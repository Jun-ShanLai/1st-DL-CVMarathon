{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習內容』\n",
    "#### 運用這幾天所學觀念搭建一個CNN分類器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習目的』\n",
    "  #### 熟悉CNN分類器搭建步驟與原理\n",
    "  #### 學員們可以嘗試不同搭法，如使用不同的Maxpooling層，用GlobalAveragePooling取代Flatten等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) #(50000, 32, 32, 3)\n",
    "\n",
    "## Normalize Data\n",
    "def normalize(X_train,X_test):\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7) \n",
    "        return X_train, X_test,mean,std\n",
    "    \n",
    "    \n",
    "## Normalize Training and Testset    \n",
    "x_train, x_test,mean_train,std_train = normalize(x_train, x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## OneHot Label 由(None, 1)-(None, 10)\n",
    "## ex. label=2,變成[0,0,1,0,0,0,0,0,0,0]\n",
    "one_hot=OneHotEncoder()\n",
    "y_train=one_hot.fit_transform(y_train).toarray()\n",
    "y_test=one_hot.transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1213 09:33:33.809697 18220 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1213 09:33:33.821667 18220 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1213 09:33:33.823642 18220 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1213 09:33:33.841594 18220 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1213 09:33:33.842592 18220 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1213 09:33:34.579690 18220 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1213 09:33:34.670447 18220 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=100)`\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
      "W1213 09:33:34.697869 18220 deprecation_wrapper.py:119] From C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1213 09:33:34.768682 18220 deprecation.py:323] From C:\\Users\\user\\Anaconda3\\envs\\Tesseract\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 1.3973 - acc: 0.5104\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.9787 - acc: 0.6559\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.7937 - acc: 0.7219\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.6579 - acc: 0.7712E\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.5403 - acc: 0.81041s - \n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.4272 - acc: 0.8512\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.3260 - acc: 0.8869\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.2490 - acc: 0.9139\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.1911 - acc: 0.9337\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.1558 - acc: 0.9456\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.1144 - acc: 0.9605\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.1091 - acc: 0.9627\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.1065 - acc: 0.9632\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.0960 - acc: 0.9672\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0755 - acc: 0.9735\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0754 - acc: 0.9742\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0748 - acc: 0.9737\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.0595 - acc: 0.9793\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.0564 - acc: 0.9808\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.0700 - acc: 0.9762\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 11s 212us/step - loss: 0.0622 - acc: 0.9788\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.0521 - acc: 0.9824\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.0400 - acc: 0.9870\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0471 - acc: 0.9841\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0489 - acc: 0.9834\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.0527 - acc: 0.9829\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0427 - acc: 0.9860\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.0463 - acc: 0.9851\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.0405 - acc: 0.9865\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 0.0472 - acc: 0.9836\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.0423 - acc: 0.9859\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 11s 212us/step - loss: 0.0350 - acc: 0.9884\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.0350 - acc: 0.9885\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0360 - acc: 0.98804s - loss - ETA: 0s - loss: 0.034\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0303 - acc: 0.9898\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0427 - acc: 0.9867\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0378 - acc: 0.9874\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0304 - acc: 0.99010s - loss: 0.0295 \n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0293 - acc: 0.99072s - loss - ETA: 0s - loss: 0.0284 - acc: 0.99 - ETA: 0s - loss: 0.0286 - ac\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.0254 - acc: 0.9917\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0289 - acc: 0.9908\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0297 - acc: 0.9901\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0377 - acc: 0.9881\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.0267 - acc: 0.99121s - loss: 0.0268 - acc: 0.99 - ETA: 1s - \n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0286 - acc: 0.9910\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.0229 - acc: 0.9923\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0240 - acc: 0.9920\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.0273 - acc: 0.9910\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.0298 - acc: 0.9903\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0286 - acc: 0.9908\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0239 - acc: 0.99251s - loss: 0\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0222 - acc: 0.9929\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0238 - acc: 0.9925\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0290 - acc: 0.99121s - loss: 0.0294 - acc: 0.991 - ETA: 1s - loss: 0.0\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0257 - acc: 0.9920\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0187 - acc: 0.9942\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0189 - acc: 0.9938\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0212 - acc: 0.99290s - loss: 0.0206 - acc: 0.99 - ETA: 0s - loss: 0.0208\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0300 - acc: 0.9910\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0193 - acc: 0.99371s - loss: \n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0212 - acc: 0.9935\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0236 - acc: 0.99252s - loss: 0. - ETA: 1s \n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0212 - acc: 0.9934\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0181 - acc: 0.9945\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0153 - acc: 0.99480s - loss: 0.0149 -\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0155 - acc: 0.99540s - loss: 0.014\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0190 - acc: 0.9942\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0281 - acc: 0.9915\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0200 - acc: 0.9935\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0148 - acc: 0.9951\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0167 - acc: 0.9947\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0132 - acc: 0.9957\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0221 - acc: 0.9932\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 0.0255 - acc: 0.9923\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.0156 - acc: 0.99500s - loss: 0.0156 - acc: 0.99\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0120 - acc: 0.9964\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.0219 - acc: 0.9934\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0179 - acc: 0.9946\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.0129 - acc: 0.99601s - loss:\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.0170 - acc: 0.9949\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0197 - acc: 0.9940\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0166 - acc: 0.9949\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 0.0182 - acc: 0.9943\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0217 - acc: 0.9932\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0127 - acc: 0.99581s - loss: 0.0121 - acc: 0.99 - ETA: 1s - loss: 0.0\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0139 - acc: 0.9956\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0198 - acc: 0.9943\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0174 - acc: 0.99501s - lo\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0118 - acc: 0.9963\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0094 - acc: 0.9969\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.0115 - acc: 0.9961\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0148 - acc: 0.99531s - los\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.0181 - acc: 0.9944\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0144 - acc: 0.99564s -\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0125 - acc: 0.9963\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 11s 211us/step - loss: 0.0167 - acc: 0.9955\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 11s 212us/step - loss: 0.0148 - acc: 0.9955\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 10s 209us/step - loss: 0.0126 - acc: 0.9959\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.0138 - acc: 0.9955\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.0110 - acc: 0.99640s - loss: 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fdc4e84048>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=Sequential()\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Convolution2D(input_shape=(32, 32, 3), kernel_size=(3,3), filters=32, activation='relu', padding='same'))#32,3,3,input_shape=(32,32,3),activation='relu''\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "'''自己決定MaxPooling2D放在哪裡'''\n",
    "#classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Convolution2D(kernel_size=(3,3), filters=32, activation='relu', padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#flatten\n",
    "classifier.add(Flatten())\n",
    "\n",
    "#FC\n",
    "classifier.add(Dense(output_dim=100, activation='relu')) #output_dim=100,activation=relu\n",
    "\n",
    "#輸出\n",
    "classifier.add(Dense(output_dim=10,activation='softmax'))\n",
    "\n",
    "\n",
    "#超過兩個就要選categorical_crossentrophy\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(x_train,y_train,batch_size=100,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測新圖片，輸入影像前處理要與訓練時相同\n",
    "\n",
    "### ((X-mean)/(std+1e-7) ):這裡的mean跟std是訓練集的\n",
    "\n",
    "# 維度如下方示範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = (np.zeros(shape=(1,32,32,3))-mean_train)/(std_train+1e-7) \n",
    "y_pred = classifier.predict(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 120us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.4256387134552, 0.6651]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
